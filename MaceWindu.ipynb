{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master Jedi Notebook\n",
    "\n",
    "### This notebook analyzes information about items, users, credits, etc from your AGO organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to connect to your GIS and get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination feature layers that will be update\n",
    "\n",
    "# Configure this after they have been created\n",
    "# pattern is \"name\": itemID\n",
    "# such as \"Example\": \"123456789abcdefghijklmnopqrstu\"\n",
    "\n",
    "outputHFLs = {\"PubEditItems\": \"\",\n",
    "              \"CreatedModifiedItems\": \"\",\n",
    "              \"WeeklyCreditHistory\": \"\",\n",
    "              \"AGO_LoginAnalysis\": \"\",\n",
    "              \"CreditsRemaining\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from arcgis.gis import GIS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "gis = GIS(\"home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's grab all the information we can about all of the items in the organization to analyze\n",
    "\n",
    "MaxSearch = 9999  # Change the maximum number of items returned for testing purposes\n",
    "\n",
    "allItems = gis.content.search(query=\"\", max_items=MaxSearch)\n",
    "\n",
    "allItemsDF = pd.DataFrame(allItems)\n",
    "\n",
    "auditLen = len(allItems)\n",
    "if auditLen >= MaxSearch:\n",
    "    print(\"The maximum number of records has been reached.\")\n",
    "print(\"Found {} items to asses\".format(auditLen))\n",
    "\n",
    "allItemsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write various results to a hosted, published *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.features import FeatureLayerCollection\n",
    "def Results2AGO(csvName, df):\n",
    "\n",
    "    # Create a temporary *.csv\n",
    "    import os, tempfile\n",
    "    tempdir = tempfile.TemporaryDirectory(suffix=None, prefix=None, dir=None)\n",
    "    print (tempdir)\n",
    "\n",
    "    ### if executing manually, you can input the item name\n",
    "    # csvName = input(\"What do you want to call the csv? \")\n",
    "\n",
    "    csvpath = (os.path.join(tempdir.name, csvName + \".csv\"))\n",
    "    print (csvpath)\n",
    "    df.to_csv(csvpath, index=False)\n",
    "\n",
    "    #Find out if it is already in AGO\n",
    "    prevCSV = gis.content.search(query=csvName, item_type=\"Feature Layer Collection\", max_items = 1)\n",
    "    if prevCSV:\n",
    "        print(\"Found a previous csv in AGO\")\n",
    "        prevCSV = prevCSV[0]\n",
    "        display(prevCSV)\n",
    "       \n",
    "        prevCSVcollection = FeatureLayerCollection.fromitem(prevCSV)\n",
    "        print(\"The table already exists in AGO\")\n",
    "\n",
    "        # We will now replace the old data with new\n",
    "        prevCSVcollection.manager.overwrite(csvpath)\n",
    "        display\n",
    "\n",
    "    else:\n",
    "        print(\"We have to upload it\")\n",
    "        csv_item = gis.content.add({}, csvpath)\n",
    "        display(csv_item)\n",
    "        csv_lyr = csv_item.publish()\n",
    "        display(csv_lyr)\n",
    "\n",
    "    tempdir.cleanup()\n",
    "    del csvName\n",
    "    print (\"The CSV has been published.\\nAll done, thanks for playing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check if it is a service that allows public editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.features import FeatureLayerCollection\n",
    "\n",
    "PubEditing = []\n",
    "\n",
    "for i in allItems:\n",
    "    if i.access == 'public':\n",
    "        if i.type == 'Feature Service':\n",
    "            try:\n",
    "                i_flc = FeatureLayerCollection.fromitem(i)\n",
    "                if 'Editing' in i_flc.properties.capabilities:\n",
    "                    PubEditing.append(i.title)\n",
    "            except:\n",
    "                print(\"Wasn't able to assess {}\".format(i.title))\n",
    "print(PubEditing)\n",
    "\n",
    "if len(PubEditing) > 0:\n",
    "    print(\"Found {} items with public editing allowed\".format(len(PubEditing)))\n",
    "    dict = {'Public Editing Items': PubEditing}  \n",
    "    PEdf = pd.DataFrame(dict)\n",
    "    \n",
    "    # Now let's create/update a *.csv to hold this information\n",
    "    print(\"Throwing it over to the csv writer/updater\")\n",
    "    Results2AGO(outputHFLs[\"PubEditItems\"], PEdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find created and updated dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "from requests.compat import urljoin\n",
    "\n",
    "CreateDates = []\n",
    "ModifiedDates =[]\n",
    "hyperlinkField = []\n",
    "\n",
    "# change your base arcgis organiztion url here\n",
    "urlBase='https://dublinohio.maps.arcgis.com/home/item.html'\n",
    "\n",
    "for item in allItems:\n",
    "    CreateDate = datetime.fromtimestamp(item[\"created\"]/1000) # Get the date created in human readable date format\n",
    "    #print(item.title, \"created on: {}\".format(CreateDate))\n",
    "    CreateDates.append(CreateDate)\n",
    "    ModifiedDate = datetime.fromtimestamp(item[\"modified\"]/1000)\n",
    "    #print(item.title, \"modified on: {}\".format(ModifiedDate))\n",
    "    ModifiedDates.append(ModifiedDate)\n",
    "    \n",
    "    urlQuery = '?id=' + item.id\n",
    "    url_full = urljoin(urlBase,urlQuery)\n",
    "    #print(item.title, \"accessed here: {}\".format(url_full))\n",
    "    hyperlinkField.append(url_full)\n",
    "\n",
    "print(\"Created {} created dates\".format(len(CreateDates)))\n",
    "print(\"Created {} modified dates\".format(len(ModifiedDates)))\n",
    "print(\"Created {} hyperlinks\".format(len(hyperlinkField)))\n",
    "\n",
    "#### Now that we have lists for all the dates and a hyperlink, let's add them to a new dataframe\n",
    "\n",
    "# this creates a new dataframe from the original one, but with less columns\n",
    "createdColumns = ['id','title','name','owner','type','size','created','modified',]\n",
    "createdDF = allItemsDF[allItemsDF.columns.intersection(createdColumns)]\n",
    "\n",
    "createdDF.insert(3, \"createdDate\", CreateDates, True) # adds and populates a created field as date\n",
    "createdDF.insert(5, \"modifiedDate\", ModifiedDates, True) # adds and populates a modified field as date\n",
    "createdDF['Hyperlink_field'] = hyperlinkField # adds a hyperlink field to the end\n",
    "\n",
    "createdDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calls the hosted *.csv portion\n",
    "### BE SURE TO RUN THAT FIRST\n",
    "\n",
    "if len(createdDF) > 0:\n",
    "    # Now let's create/update a *.csv to hold this information\n",
    "    print(\"Throwing it over to the csv writer/updater\")\n",
    "    Results2AGO(outputHFLs[\"CreatedModifiedItems\"], createdDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This gathers the credit usage by category for the past X time (for instance, 7 days)\n",
    "\n",
    "cm = gis.admin.credits\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from arcgis.features import FeatureLayer\n",
    "import pandas as pd\n",
    "\n",
    "tDay = datetime.now().date()\n",
    "\n",
    "# It is maybe silly to start with a date object, convert it into components and then reconstruct a date\n",
    "# However, that is how it must be done, confirmed through a support ticket with Esri\n",
    "lastWeek = tDay - timedelta(days=7) # this is where the time interval is set\n",
    "thisDay = int(tDay.strftime(\"%d\"))\n",
    "thisMonth = int(tDay.strftime(\"%m\"))\n",
    "thisYr = int(tDay.strftime(\"%Y\"))\n",
    "lastDay = int(lastWeek.strftime(\"%d\"))\n",
    "lastMonth = int(lastWeek.strftime(\"%m\"))\n",
    "lastYr = int(lastWeek.strftime(\"%Y\"))\n",
    "\n",
    "start_date = datetime(lastYr, lastMonth, lastDay)\n",
    "end_date7 = datetime(thisYr, thisMonth, thisDay)\n",
    "\n",
    "print(start_date)\n",
    "print(end_date7)\n",
    "\n",
    "# This is where the actual credit usage is returned\n",
    "credits7 = cm.credit_usage(start_time=end_date7, end_time=start_date)\n",
    "print(credits7)\n",
    "\n",
    "# Variables of the various credit usage types are assigned and checked for returns\n",
    "DayEnding = tDay\n",
    "applogin = credits7.get('applogin') if credits7.get('applogin') else 0\n",
    "features = credits7.get('features')\n",
    "geoenrich = credits7.get('geoenrich') if credits7.get('geoenrich') else 0\n",
    "intnotebks = credits7.get('intnotebks')\n",
    "notebooks = credits7.get('notebooks')\n",
    "portal = credits7.get('portal')\n",
    "schdnotebks = credits7.get('schdnotebks')\n",
    "spanalysis = credits7.get('spanalysis')\n",
    "tiles = credits7.get('tiles')\n",
    "vectortiles = credits7.get('vectortiles')\n",
    "\n",
    "# We will read in the existing data in order to retain it\n",
    "# add in the url of the actual table like below...\n",
    "table_url =\"https://services1.arcgis.com/notreal/arcgis/rest/services/WeeklyCreditHistory/FeatureServer/0\"\n",
    "tbl = FeatureLayer(table_url, gis=gis)\n",
    "\n",
    "oldWCHdf = tbl.query(where='1=1',as_df='true') #querying without any conditions returns all the features\n",
    "\n",
    "# Now we will add a record for the new data to the already loaded prior data\n",
    "newWCHdf = oldWCHdf.append({\n",
    "'DayEnding':tDay,\n",
    "'applogin':applogin,\n",
    "'features':features,\n",
    "'geoenrich':geoenrich,\n",
    "'intnotebks':intnotebks,\n",
    "'notebooks':notebooks,\n",
    "'portal':portal,\n",
    "'schdnotebks':schdnotebks,\n",
    "'spanalysis':spanalysis,\n",
    "'tiles':tiles,\n",
    "'vectortiles':vectortiles,\n",
    "'ObjectId':oldWCHdf[\"ObjectId\"].max()+1\n",
    "}, ignore_index=True)\n",
    "\n",
    "newWCHdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calls the hosted *.csv portion\n",
    "### BE SURE TO RUN THAT FIRST\n",
    "\n",
    "if len(newWCHdf) > 0:\n",
    "    # Now let's create/update a *.csv to hold this information\n",
    "    print(\"Throwing it over to the csv writer/updater\")\n",
    "    \n",
    "    Results2AGO(outputHFLs[\"WeeklyCreditHistory\"], newWCHdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGO Logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can report on how people are logging into ArcGIS Online\n",
    "### This is not only who is logging in, but also through what app\n",
    "### This can be arcgis.com, QuickCapture, dashboards, etc.\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# We need to set up the dates for which we're going to analyze\n",
    "tDay = datetime.now().date()\n",
    "lastWeek = tDay - timedelta(days=2)\n",
    "thisDay = int(tDay.strftime(\"%d\"))\n",
    "thisMonth = int(tDay.strftime(\"%m\"))\n",
    "thisYr = int(tDay.strftime(\"%Y\"))\n",
    "lastDay = int(lastWeek.strftime(\"%d\"))\n",
    "lastMonth = int(lastWeek.strftime(\"%m\"))\n",
    "lastYr = int(lastWeek.strftime(\"%Y\"))\n",
    "# print(tDay)\n",
    "# print(lastWeek)\n",
    "# print(thisDay)\n",
    "# print(thisMonth)\n",
    "# print(thisYr)\n",
    "# print(lastDay)\n",
    "# print(lastMonth)\n",
    "# print(lastYr)\n",
    "\n",
    "start_date = datetime(thisYr, thisMonth, thisDay)\n",
    "to_date = datetime(lastYr, lastMonth, lastDay)\n",
    "print(to_date)\n",
    "print(start_date)\n",
    "\n",
    "# retreive the data\n",
    "Agologins = gis.admin.history(to_date, start_date, num = 9999, data_format='df')\n",
    "\n",
    "# drop columns for security and extraneous data\n",
    "Agologins.drop(['id','idType','orgId','ip','reqId','owner','data'], axis = 1, inplace = True)\n",
    "\n",
    "localTimes = []\n",
    "\n",
    "import datetime\n",
    "for index, row in Agologins.iterrows():\n",
    "    #print(row['created'])\n",
    "    localTime = datetime.datetime.fromtimestamp(int(row['created'])/1000)\n",
    "    localTimes.append(localTime.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "Agologins['CreatedDate'] = localTimes\n",
    "\n",
    "splitcolumnns = ['actor','appId']\n",
    "\n",
    "AGOloginAnalysis = Agologins.groupby(splitcolumnns).agg(appIdCount = (\"appId\", \"count\"))\n",
    "\n",
    "# reset index because of multi-level indexing from groupby made for weird things. \n",
    "AGOloginAnalysis.reset_index(inplace=True)\n",
    "AGOloginAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calls the hosted *.csv portion\n",
    "### BE SURE TO RUN THAT FIRST\n",
    "\n",
    "if len(AGOloginAnalysis) > 0:\n",
    "    # Now let's create/update a *.csv to hold this information\n",
    "    print(\"Throwing it over to the csv writer/updater\")\n",
    "    #csvName = \n",
    "    Results2AGO(outputHFLs['AGO_LoginAnalysis'], AGOloginAnalysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits Remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This will return how many credits are left\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "tday = datetime.now()\n",
    "\n",
    "creditsleft = gis.admin.credits.credits\n",
    "print(creditsleft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now that we know how many credits are remaining\n",
    "### we should add this to the hosted table\n",
    "\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# We will read in the existing data in order to retain it\n",
    "# add in the url of the actual table like below...\n",
    "table_url = \"https://services1.arcgis.com/notreal/arcgis/rest/services/CreditsRemaining/FeatureServer/0\"\n",
    "tbl = FeatureLayer(table_url, gis=gis)\n",
    "\n",
    "import pandas as pd\n",
    "oldCRdf = tbl.query(where='1=1',as_df='true') #querying without any conditions returns all the features\n",
    "#oldCRdf\n",
    "\n",
    "# Now we will add a record for the new data to the already loaded prior data\n",
    "newCRdf = oldCRdf.append({\n",
    "        'Tday':tday,\n",
    "        'CreditsRemaining':creditsleft,\n",
    "        'ObjectId':oldCRdf[\"ObjectId\"].max()+1\n",
    "        }, ignore_index=True)\n",
    "\n",
    "newCRdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calls the hosted *.csv portion\n",
    "### BE SURE TO RUN THAT FIRST\n",
    "\n",
    "if len(newCRdf) > 0:\n",
    "    # Now let's create/update a *.csv to hold this information\n",
    "    print(\"Throwing it over to the csv writer/updater\")\n",
    "    #csvName = \"AllItems\"\n",
    "    Results2AGO(outputHFLs['CreditsRemaining'], newCRdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "esriNotebookRuntime": {
   "notebookRuntimeName": "ArcGIS Notebook Python 3 Standard",
   "notebookRuntimeVersion": "5.0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
